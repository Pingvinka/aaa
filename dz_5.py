# -*- coding: utf-8 -*-
"""Dz_5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y62YH1hE5it0GvLRFeknomo9ofP1eVtI

Краткое содержание:

1) ml библиотеки:
- pycaret для первичного анализа моделей

- catboost

- sklearn

- lightgbm

- xgboost

2) Polars я не юзала

3) Использовала меньше 20 признаков

4) Корреляция между признаками меньше 0.95 по модулю

5) Private score в лб

#Загрузка
"""

!pip install catboost

!pip install optuna

import pandas as pd
from datetime import datetime, timedelta
import numpy as np
from scipy.stats import ks_2samp
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm.notebook import tqdm
import catboost
import lightgbm as lgb
import gc
from sklearn.neighbors import NearestNeighbors

from catboost import CatBoostRegressor
from sklearn.model_selection import train_test_split

from sklearn.ensemble import ExtraTreesRegressor
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor

"""#Бейзлайн"""

train = pd.read_csv('/content/train.csv')
sub = pd.read_csv('/content/sample_submission.csv')

sub['date'] = sub['id'].apply(lambda x:x.split('_')[0])
sub['id_house'] = sub['id'].apply(lambda x:int(x.split('_')[1]))
sub['date'] = pd.to_datetime(sub['date'])
train['date'] = pd.to_datetime(train['date'])

date_range = pd.date_range(train['date'].min(), sub['date'].max(), freq = 'MS').tolist()
city_list = []
time_list = []
for city in sub['id_house'].unique():
    time_list += date_range
    city_list += [city] * len(date_range)
data = pd.DataFrame()
data['id_house'] = city_list
data['date'] = time_list
data = data.merge(train, on = ['id_house', 'date'], how = 'left')
data_other_columns = [x for x in data.columns if x not in ['id_house', 'date']]
#fill issing data
for col in tqdm(data_other_columns):
    data[col] = data.groupby('id_house', group_keys = False)[col].apply(lambda x:x.fillna(method = 'bfill').fillna(method = 'ffill'))

data['preds'] = -1.

coords_data = data.drop_duplicates('id_house')[['lat', 'lng']]
nn = NearestNeighbors(n_neighbors = 15)
nn.fit(coords_data.values)

col = 'med_price'
dict_vals = data.groupby(['id_house', 'date'])[col].mean().to_dict()
dict_ind_house = {i:k for i, k in enumerate(data['id_house'].unique())}
list_month = data['date'].unique()

dict_mean = {}
dict_std = {}
for neighbors in nn.kneighbors(coords_data.values)[1]:
    for month in list_month:
        vals = []
        for neighbor in neighbors:
            if (dict_ind_house[neighbor], month) in dict_vals:
                vals += [dict_vals[(dict_ind_house[neighbor], month)]]
        if len(vals) > 0:
            dict_mean[(dict_ind_house[neighbors[0]], month)] = np.mean(vals)

data['med_price_mean'] = [dict_mean.get((c, m), None) for c,m in data[['id_house', 'date']].values]

def catboost_train(train, target, split_list, param):

    bst_list = []
    for i , (train_index, val_index, test_index) in enumerate(split_list):

        tr = catboost.Pool(train[train_index], label = target[train_index])
        te = catboost.Pool(train[val_index], label = target[val_index])

        bst = catboost.train(tr, param, eval_set = te, iterations = 1000, early_stopping_rounds = 100, verbose =300)
        bst_list += [bst]

        gc.collect()
        del tr, te

    return bst_list
#{'iterations': 1381, 'depth': 10, 'learning_rate': 0.03374541958964185, 'l2_leaf_reg': 9.110469859305407, 'random_strength': 0.1481241935364265, 'bagging_temperature': 9.216998599698078}
params_cat = {
    'loss_function' :'MAE',
    #'max_depth' : 5,
    'eval_metric' :'MAPE',
    #'learning_rate' : .1,
    #'l2_leaf_reg' : 15,
    'random_state' : 42,
    'depth': 7,
    'learning_rate': 0.0789689619229723,
    'l2_leaf_reg': 0.81255450881796506,
    'random_strength': 0.00010484300892358066,
    'bagging_temperature': 9.821202731217099
    }


#Хотела я оптимизировать каждую модель по отдельности, но времени не хватит, поздновато я за дзшку села, хех :>
def lgb_train(train, target, split_list, param):

    bst_list = []
    for i , (train_index, val_index, test_index) in enumerate(split_list):

        tr = lgb.Dataset(train[train_index], target[train_index])
        te = lgb.Dataset(train[val_index], target[val_index], reference=tr)

        bst = lgb.train(param, tr, num_boost_round = 1000, valid_sets = te,
                        callbacks = [lgb.early_stopping(100), lgb.log_evaluation(5000)])
        bst_list += [bst]

        gc.collect()
        del tr, te

    return bst_list

params_lgb = {
    'objective':        'mae',
    'verbosity':        -1,
    'boosting_type':    'gbdt',
    'metric' : 'mape',
    'lambda_l1':        5,
    'learning_rate':    0.1,
    'num_leaves':        16,
    'random_state': 42
}

def et_train(train, target, split_list, param):
    model_list = []
    for i, (train_index, val_index, test_index) in enumerate(split_list):
        model = ExtraTreesRegressor(
            n_estimators=param['n_estimators'],
            max_depth=param['max_depth'],
            min_samples_split=param['min_samples_split'],
            min_samples_leaf=param['min_samples_leaf'],
            max_features=param['max_features'],
            bootstrap=param['bootstrap'],
            n_jobs=param['n_jobs'],
            random_state=param['random_state']
        )

        model.fit(train[train_index], target[train_index])

        val_pred = model.predict(train[val_index])
        score = mean_absolute_error(target[val_index], val_pred)
        print(f'et val split {i}: {score:.4f}')

        model_list.append(model)
        gc.collect()

    return model_list

params_et = {
    'n_estimators': 200,
    'max_depth': None,
    'min_samples_split': 5,
    'min_samples_leaf': 2,
    'max_features': 'sqrt',
    'bootstrap': False,
    'n_jobs': -1,
    'random_state': 42
}

def rf_train(train, target, split_list, param):
    model_list = []
    for i, (train_index, val_index, test_index) in enumerate(split_list):
        model = RandomForestRegressor(
            n_estimators=param['n_estimators'],
            max_depth=param['max_depth'],
            min_samples_split=param['min_samples_split'],
            min_samples_leaf=param['min_samples_leaf'],
            max_features=param['max_features'],
            n_jobs=param['n_jobs'],
            random_state=param['random_state']
        )
        model.fit(train[train_index], target[train_index])

        # Validation
        val_pred = model.predict(train[val_index])
        score = mean_absolute_error(target[val_index], val_pred)
        print(f'rf val split {i}: {score:.4f}')

        model_list.append(model)
        gc.collect()

    return model_list

params_rf = {
    'n_estimators': 300,
    'max_depth': 8,
    'min_samples_split': 8,
    'min_samples_leaf': 4,
    'max_features': 0.7,
    'n_jobs': -1,
    'random_state': 42
}

def xgb_train(train, target, split_list, param):
    model_list = []
    for i, (train_index, val_index, test_index) in enumerate(split_list):
        model = XGBRegressor(
            **param,
            eval_metric='mae',
            early_stopping_rounds=50
        )
        model.fit(
            train[train_index], target[train_index],
            eval_set=[(train[val_index], target[val_index])],
            verbose=0
        )

        model_list.append(model)
        gc.collect()

    return model_list

params_xgb = {
    'objective': 'reg:absoluteerror',
    'learning_rate': 0.05,
    'n_estimators': 2000,
    'max_depth': 7,
    'subsample': 0.8,
    'colsample_bytree': 0.7,
    'gamma': 0.5,
    'reg_alpha': 1.0,
    'reg_lambda': 1.5,
    'random_state': 42,
    'tree_method': 'hist',
    'enable_categorical': False
}

train_month = sorted(train['date'].unique())
test_month = sorted(sub['date'].unique())


for m_predict in [1, 2, 3, 4]:
    data[f'mean_price_shift_{m_predict}'] = data.groupby('id_house', group_keys = False)['mean_price'].apply(lambda x:x.shift(m_predict))
    data['new_target'] = data['mean_price'] / data[f'mean_price_shift_{m_predict}']

    data['year'] = data['date'].dt.year

    if m_predict in [1, 2, 3]:
      train_cols_new = ['skolko_let'] #Более информативен, чем year. Признак, сколько лет дому (текущий год - год постройки)
      for col in tqdm(['vc_city_quadkey', 'number_total', 'new_target', 'mean_price',
                      'num_builds_live', 'room_one', 'room_two', 'shopping_per_flat', 'area_price', 'med_price_mean', 'price_diff']):
          data[f'{col}_shift_rm_{m_predict}'] = data.groupby('id_house', group_keys = False)[col].apply(lambda x:
                                              x.rolling(3, min_periods = 1).mean().shift(m_predict))
          train_cols_new += [f'{col}_shift_rm_{m_predict}']
      split_list = []
      list_val_months = [-1, -2]
      for val_month in list_val_months:
          train_index = data[(data['date'] > train_month[5]) & (data['date'] <= train_month[val_month - 1])].index
          val_index = data[data['date'] == train_month[val_month]].index
          test_index = data[data['date'] == test_month[m_predict - 1]].index
          split_list += [(train_index, val_index, test_index)]
    #train_cols = selected_features
    #data = new_data.copy()
    # CHECKING CORRELATIONS

    train_cols_best = ['med_price_noised', 'new_med_price', 'area_price', 'luxury_housing', 'high_mean_price', 'price_diff', 'med_price_mean', 'mean_area', 'price_ratio', 'lng', 'vc_city_quadkey', 'lng_exp', 'total_cnt', 'rooms_area', 'beauty_per_flat', 'median_build_year_area', 'skolko_let', 'shopping_per_flat']
    trains = {
      1: train_cols_new,
      2: train_cols_new,
      3: train_cols_new,
      4: train_cols_best
      }
    train_cols = trains[m_predict]

    print('LEN:', len(train_cols))
    vals = data[train_cols].corr().abs().values
    print('CHECK CORR COLS: ', bool(vals[~np.eye(vals.shape[0],dtype=bool)].max() < 0.95) == True,
          'MAX CORR: ', vals[~np.eye(vals.shape[0],dtype=bool)].max())



    bst_list_catboost = catboost_train(data[train_cols].values, data['new_target'].values, split_list, params_cat)

    catboost_preds = []
    for num_, bst in enumerate(bst_list_catboost):
        val_index = split_list[num_][-2]
        val_pred = bst.predict(data[train_cols].values[val_index]) * data[f'mean_price_shift_{m_predict}'][val_index]
        score = (data['mean_price'][val_index] - val_pred).abs().mean()
        print(f'VAL SCORE CATBOOST MONTH {num_}: ', score)
        test_index = split_list[num_][-1]

        catboost_preds += [ bst.predict(data[train_cols].values[test_index]) * data[f'mean_price_shift_{m_predict}'][test_index] ]
    catboost_preds = np.mean(catboost_preds, 0)

    bst_list_lgb = lgb_train(data[train_cols].values, data['new_target'].values, split_list, params_lgb)
    lgb_preds = []
    for num_, bst in enumerate(bst_list_lgb):
        val_index = split_list[num_][-2]
        val_pred = bst.predict(data[train_cols].values[val_index]) * data[f'mean_price_shift_{m_predict}'][val_index]
        score = (data['mean_price'][val_index] - val_pred).abs().mean()
        print(f'VAL SCORE LIGHTGBM MONTH {num_}: ', score)
        test_index = split_list[num_][-1]
        lgb_preds += [ bst.predict(data[train_cols].values[test_index]) * data[f'mean_price_shift_{m_predict}'][test_index] ]
    lgb_preds = np.mean(lgb_preds, 0)

    bst_list_et = et_train(data[train_cols].values, data['new_target'].values, split_list, params_et)
    et_preds = []
    for num_, bst in enumerate(bst_list_et):
        val_index = split_list[num_][-2]
        val_pred = bst.predict(data[train_cols].values[val_index]) * data[f'mean_price_shift_{m_predict}'][val_index]
        score = (data['mean_price'][val_index] - val_pred).abs().mean()
        print(f'VAL SCORE EXTRATREES MONTH {num_}: ', score)
        test_index = split_list[num_][-1]
        et_preds += [ bst.predict(data[train_cols].values[test_index]) * data[f'mean_price_shift_{m_predict}'][test_index] ]
    et_preds = np.mean(et_preds, 0)

    bst_list_xgb = xgb_train(data[train_cols].values, data['new_target'].values, split_list, params_xgb)
    xgb_preds = []
    for num_, bst in enumerate(bst_list_xgb):
        val_index = split_list[num_][-2]
        val_pred = bst.predict(data[train_cols].values[val_index]) * data[f'mean_price_shift_{m_predict}'][val_index]
        score = (data['mean_price'][val_index] - val_pred).abs().mean()
        print(f'VAL SCORE XGBOOST MONTH {num_}: ', score)
        test_index = split_list[num_][-1]
        xgb_preds += [bst.predict(data[train_cols].values[test_index]) * data[f'mean_price_shift_{m_predict}'][test_index] ]
    xgb_preds = np.mean(xgb_preds, 0)


    bst_list_rf = rf_train(data[train_cols].values, data['new_target'].values, split_list, params_rf)
    rf_preds = []
    for num_, bst in enumerate(bst_list_rf):
        val_index = split_list[num_][-2]
        val_pred = bst.predict(data[train_cols].values[val_index]) * data[f'mean_price_shift_{m_predict}'][val_index]
        score = (data['mean_price'][val_index] - val_pred).abs().mean()
        print(f'VAL SCORE RANDOMFOREST MONTH {num_}: ', score)
        test_index = split_list[num_][-1]
        rf_preds += [bst.predict(data[train_cols].values[test_index]) * data[f'mean_price_shift_{m_predict}'][test_index] ]
    rf_preds = np.mean(rf_preds, 0)


    data.loc[test_index, 'preds'] = (catboost_preds + lgb_preds + xgb_preds + rf_preds + et_preds) / 5

sub=sub.drop(columns=['preds'])

sub = sub.merge(data[['date', 'id_house', 'preds']], on = ['date', 'id_house'], how = 'left')

#sub = sub.drop(columns=['preds_y', 'preds_x'])
sub

sub['target'] = sub['preds']
# delete temp files of catboost from working folder
sub[['id', 'target']].to_csv('solution.csv', index = None)
sub['target'].min(), sub['target'].mean(), sub['target'].isnull().sum()

from google.colab import files
files.download("solution.csv")

data.columns

#train_features = ['apart_to_room', 'num_builds_live', 'med_price', 'room_four']
vals = data[train_cols_new].corr().abs().values
print(vals)
bool(vals[~np.eye(vals.shape[0],dtype=bool)].max() < 0.95) == True

"""#Генерация и отбор признаков"""

!pip install geopy

from geopy.geocoders import Nominatim
from geopy.exc import GeocoderTimedOut
from sklearn.cluster import KMeans
from geopy.distance import geodesic
from sklearn.model_selection import KFold

def aaa(df):

  for column in ['room_three', 'room_four', 'room_two', 'room_one','room_zero', 'lng', 'lat']:
      df[f'{column}_log'] = np.log1p(df[column])

  for column in ['room_three', 'room_four', 'room_two', 'room_one','room_zero', 'lng', 'lat']:
      df[f'{column}_exp'] = np.exp(df[column])

  for column in ['room_three', 'room_four', 'room_two', 'room_one','room_zero', 'lng', 'lat']:
      df[f'{column}_sqrt'] = np.sqrt(df[column])

  for column in ['room_three', 'room_four', 'room_two', 'room_one','room_zero', 'lng', 'lat']:
      df[f'{column}_squared'] = df[column] ** 2

  for col in ['vc_city_quadkey']:
      freq_encoding = df[col].value_counts(normalize=True)
      df[f'{col}_freq'] = df[col].map(freq_encoding)

  noise = np.random.normal(0, df['med_price'].std() * 0.3, len(df)) #Добавляем шум чисто, чтоб корреляция меньше была
  df['new_med_price'] = df['med_price'] + noise

  df['price_diff'] = df['mean_price'] - df['med_price']
  df['price_ratio'] = df['mean_price'] / df['med_price']
  df['builds_ratio'] = df['num_builds_live'] / df['num_builds_series_live']
  df['apart_per_room'] = df['apart_to_room'] / (df['room_zero'] + df['room_one'] + df['room_two'] + df['room_three'] + df['room_four'])

  df['avg_price_quadkey'] = df.groupby('vc_city_quadkey')['med_price'].transform('mean')
  df['median_build_year_area'] = df.groupby('vc_city_quadkey')['build_year_median'].transform('median')

  median_mean_area = df['mean_area'].median()
  median_mean_price = df['mean_price'].median()
  df['high_mean_area'] = (df['mean_area'] > median_mean_area).astype(int)
  df['high_mean_price'] = (df['mean_price'] > median_mean_price).astype(int)

  df['area_price'] = df['mean_area'] * df['mean_price']
  df['rooms_area'] = df['room_zero'] + df['room_one'] + df['room_two'] + df['room_three'] + df['room_four']
  df['total_cnt'] = df['healthcare_cnt'] + df['beauty_cnt'] + df['shopping_cnt']

  df['has_shopping'] = (df['shopping_cnt'] > 0).astype(int)
  df['luxury_housing'] = (df['med_price'] > df['med_price'].quantile(0.9)).astype(int)

  df['healthcare_per_flat'] = df['healthcare_cnt'] / (df['flats_cnt'] + 1e-6)
  df['shopping_per_flat'] = df['shopping_cnt'] / (df['flats_cnt'] + 1e-6)
  df['beauty_per_flat'] = df['beauty_cnt'] / (df['flats_cnt'] + 1e-6)


  moscow_center = (55.751244, 37.618423)
  df['distance_to_moscow_center'] = df.apply(lambda row: geodesic((row['lat'], row['lng']), moscow_center).km, axis=1) #Я глянула - у вас там все квартиры в Баренцевом море плавают!

  kmeans = KMeans(n_clusters=5)
  df['cluster'] = kmeans.fit_predict(df[['lat', 'lng']])

  df['date'] = pd.to_datetime(df['date'])
  #временные тут вообще лишние получились
  #df['year'] = df['date'].dt.year бесполезно тут
  df['month'] = df['date'].dt.month
  #df['day'] = df['date'].dt.day бесполезно тут
  df['day_of_week'] = df['date'].dt.dayofweek
  df['quarter'] = df['date'].dt.quarter

  #df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)  бесполезно тут
  #df['is_holiday'] = df['date'].dt.strftime('%m-%d').isin(['01-01', '07-04', '12-25']).astype(int)  бесполезно тут

  df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
  df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)

  df['skolko_let'] = df['date'].dt.year - df['build_year_median']

  '''
  df = df.sort_values(by=["id_house", "date"])
  df["prev_month_price"] = df.groupby("id_house")["med_price"].shift(1)
  df["price_diff_prev_month"] = df["med_price"] - df["prev_month_price"]
  '''
  return df

aaa(data)
data

data.isin([np.inf, -np.inf]).values.any()

numeric_df = data.select_dtypes(include=['int64', 'float64', 'int32', 'float32'])
corr_matrix = numeric_df.corr()
plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix, annot=False, cmap='coolwarm')
plt.title('Матрица корреляции')
plt.show()

corr_matrix = data[train_cols_new].corr().abs()

# Находим пары признаков с корреляцией > 0.95
high_corr_pairs = []
for i in range(len(corr_matrix.columns)):
    for j in range(i):
        if corr_matrix.iloc[i, j] > 0.95:
            col_i = corr_matrix.columns[i]
            col_j = corr_matrix.columns[j]
            high_corr_pairs.append((col_i, col_j, corr_matrix.iloc[i, j]))

# Выводим результаты
print("Высококоррелирующие пары (корреляция > 0.95):")
for pair in high_corr_pairs:
    print(f"{pair[0]} - {pair[1]}: {pair[2]:.4f}")

train_features = ['med_price_noised', 'new_med_price', 'area_price', 'luxury_housing', 'high_mean_price', 'price_diff', 'med_price_mean', 'mean_area', 'price_ratio', 'id_house', 'lng', 'vc_city_quadkey', 'lng_exp', 'total_cnt', 'rooms_area', 'beauty_per_flat', 'median_build_year_area', 'skolko_let', 'shopping_per_flat']
vals = data[train_features].corr().abs().values
print(vals)
bool(vals[~np.eye(vals.shape[0],dtype=bool)].max() < 0.95) == True

"""#Выбор нужных признаков"""

data.columns

data['year'] = data['date'].dt.year
data['month'] = data['date'].dt.month

# Сдвиг цены для прогноза на 1 месяц (можно менять)
m_predict = 1
data['mean_price_shift'] = data.groupby('id_house')['mean_price'].shift(m_predict)
data['new_target'] = data['mean_price'] / data['mean_price_shift']

# Генерация ключевых признаков (упрощённый вариант)
features = [
'id_house', 'date', 'apart_to_room', 'num_builds_live',
       'num_builds_series_live', 'room_three', 'med_price', 'room_four',
       'room_one', 'mean_area', 'room_zero', 'number_total', 'room_two',
       'vc_city_quadkey', 'healthcare_cnt', 'flats_cnt', 'beauty_cnt',
       'shopping_cnt', 'build_year_median', 'lng', 'lat', 'preds',
       'med_price_mean', 'room_three_log', 'room_four_log', 'room_two_log',
       'room_one_log', 'room_zero_log', 'lng_log', 'lat_log', 'room_three_exp',
       'room_four_exp', 'room_two_exp', 'room_one_exp', 'room_zero_exp',
       'lng_exp', 'lat_exp', 'room_three_sqrt', 'room_four_sqrt',
       'room_two_sqrt', 'room_one_sqrt', 'room_zero_sqrt', 'lng_sqrt',
       'lat_sqrt', 'room_three_squared', 'room_four_squared',
       'room_two_squared', 'room_one_squared', 'room_zero_squared',
       'lng_squared', 'lat_squared', 'vc_city_quadkey_freq', 'new_med_price',
       'price_diff', 'price_ratio', 'builds_ratio', 'apart_per_room',
       'avg_price_quadkey', 'median_build_year_area', 'high_mean_area',
       'high_mean_price', 'area_price', 'rooms_area', 'total_cnt',
       'has_shopping', 'luxury_housing', 'healthcare_per_flat',
       'shopping_per_flat', 'beauty_per_flat', 'cluster', 'month',
       'day_of_week', 'quarter', 'month_sin', 'month_cos', 'skolko_let'
]

# Удаляем пропуски после сдвигов
data_clean = data.copy()

# Разделение на train/test с учётом временной природы
train_data, test_data = train_test_split(data_clean, test_size=0.2, random_state=42)

X_train, y_train = train_data.drop(columns = 'mean_price'), train_data['mean_price']
X_test, y_test = test_data.drop(columns = 'mean_price'), test_data['mean_price']

# Инициализация и обучение CatBoost
model = CatBoostRegressor(
    iterations=1000,
    learning_rate=0.05,
    depth=6,
    verbose=100,
    early_stopping_rounds=50,
    cat_features=['year', 'high_mean_area', 'high_mean_price', 'has_shopping', 'luxury_housing', 'cluster']  # Укажите категориальные признаки
)

model.fit(X_train, y_train, eval_set=(X_test, y_test))

# Анализ важности признаков
feature_importance = model.get_feature_importance()
importance_df = pd.DataFrame({
    'Feature': data.drop(columns = 'mean_price').columns,
    'Importance': feature_importance
}).sort_values('Importance', ascending=False)

print("Топ-25 важных признаков:")
print(importance_df.head(25))

importance_df = pd.read_csv('/content/import.csv')

import pandas as pd
import numpy as np

def select_low_corr_features(importance_df, data, target_col='mean_price', n_features=20, corr_threshold=0.95):
    """
    Выбирает топ-N важных признаков с корреляцией не выше порога

    Параметры:
    - importance_df: DataFrame с колонками ['Feature', 'Importance']
    - data: исходный датасет с данными
    - target_col: название целевой переменной
    - n_features: количество отбираемых признаков
    - corr_threshold: порог максимальной допустимой корреляции
    """
    # Сортируем признаки по важности
    sorted_features = importance_df.sort_values('Importance', ascending=False)['Feature'].tolist()

    selected = []
    for feature in sorted_features:
        # Проверяем наличие признака в данных
        if feature not in data.columns:
            continue

        # Временный набор признаков для проверки
        temp_features = selected + [feature, target_col]

        # Проверяем корреляции
        corr_matrix = data[temp_features].corr().abs()
        np.fill_diagonal(corr_matrix.values, 0)  # Игнорируем диагональ

        # Если все корреляции в норме
        if corr_matrix.max().max() <= corr_threshold:
            selected.append(feature)

        # Прерываем при достижении нужного количества
        if len(selected) >= n_features:
            break

    return selected[:n_features]

# Использование функции
selected_features = select_low_corr_features(importance_df, data)

print("Отобранные признаки с низкой корреляцией:")
print(selected_features)

def select_features_with_noise(importance_df, data, target_col='mean_price',
                             n_features=20, corr_threshold=0.95, noise_strength=0.3):
    """
    Усовершенствованный отбор признаков с обработкой топ-5 через добавление шума

    Параметры:
    - noise_strength: сила шума (доля от std признака)
    """
    data_mod = data.copy()
    sorted_features = importance_df.sort_values('Importance', ascending=False)['Feature'].tolist()
    top5_features = sorted_features[:5]

    selected = []
    for idx, feature in enumerate(sorted_features):
        if feature not in data_mod.columns:
            continue

        # Для топ-5 пробуем добавлять шум при необходимости
        if feature in top5_features:
            temp_features = selected + [feature, target_col]
            current_data = data_mod[temp_features]

            # Проверяем корреляции
            corr_matrix = current_data.corr().abs()
            np.fill_diagonal(corr_matrix.values, 0)
            max_corr = corr_matrix.max().max()

            if max_corr > corr_threshold:
                # Генерируем шум
                original_values = data_mod[feature].values
                std = np.std(original_values)
                noise = np.random.normal(0, std * noise_strength, size=len(data_mod))

                # Пробуем разные уровни шума (итеративная регулировка)
                for attempt in range(3):
                    # Добавляем шум и проверяем
                    noised_values = original_values + noise * (attempt + 1)
                    data_mod[f'{feature}_noised'] = noised_values

                    # Обновляем проверочные данные
                    current_data = data_mod[selected + [f'{feature}_noised', target_col]]
                    new_corr = current_data.corr().abs().values
                    np.fill_diagonal(new_corr, 0)

                    if new_corr.max() <= corr_threshold:
                        selected.append(f'{feature}_noised')
                        print(f'Добавлен шум к {feature} (попытка {attempt+1})')
                        break
                else:
                    print(f'Не удалось снизить корреляцию для {feature}')
                    continue
            else:
                selected.append(feature)
        else:
            # Стандартная проверка для не-топовых признаков
            temp_features = selected + [feature, target_col]
            corr_matrix = data_mod[temp_features].corr().abs()
            np.fill_diagonal(corr_matrix.values, 0)

            if corr_matrix.max().max() <= corr_threshold:
                selected.append(feature)

        if len(selected) >= n_features:
            break

    return selected[:n_features], data_mod

# Использование
selected_features, new_data = select_features_with_noise(importance_df, data)

print("Итоговые признаки:")
print(selected_features)

data['med_price_noised'] =  new_data['med_price_noised']

train_data = new_data[selected_features]

new_data = pd.read_csv('lalala.csv')

new_data.drop(columns=new_data.columns[0], axis=1, inplace=True)

new_data['new_target'] = new_data['new_target'].fillna(value=1.000000)

data

"""#Catboost

"""

import pandas as pd
import numpy as np
from catboost import CatBoostRegressor
import lightgbm as lgb
from prophet import Prophet
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_absolute_error

import optuna
from optuna.samplers import TPESampler

# 1. Оптимизация CatBoost
def optimize_catboost(trial, X_train, y_train, X_val, y_val):
    params = {
        'iterations': trial.suggest_int('iterations', 400, 1800),
        'depth': trial.suggest_int('depth', 4, 10),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-3, 10.0, log=True),
        'random_strength': trial.suggest_float('random_strength', 1e-5, 10.0, log=True),
        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 10.0),
        'od_type': 'Iter',
        'loss_function': 'MAE',
        'verbose': True
    }

    model = CatBoostRegressor(**params)
    model.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=50)
    pred = model.predict(X_val)
    return mean_absolute_error(y_val, pred)

# 2. Оптимизация LightGBM
def optimize_lightgbm(trial, X_train, y_train, X_val, y_val):
    params = {
        'objective': 'mae',
        'num_leaves': trial.suggest_int('num_leaves', 16, 256),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
        'min_child_samples': trial.suggest_int('min_child_samples', 10, 200),
        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),
        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),
        'n_estimators': 1000,
        'verbosity': 1
    }

    model = lgb.LGBMRegressor(**params)
    model.fit(X_train, y_train,
              eval_set=[(X_val, y_val)],
              callbacks=[lgb.early_stopping(50)])
    pred = model.predict(X_val)
    return mean_absolute_error(y_val, pred)

# 3. Оптимизация Prophet
def optimize_prophet(trial, train_df):
    params = {
        'changepoint_prior_scale': trial.suggest_float('changepoint_prior_scale', 0.001, 0.5),
        'seasonality_prior_scale': trial.suggest_float('seasonality_prior_scale', 0.01, 10),
        'holidays_prior_scale': trial.suggest_float('holidays_prior_scale', 0.01, 10),
        'seasonality_mode': trial.suggest_categorical('seasonality_mode', ['additive', 'multiplicative'])
    }

    model = Prophet(
        yearly_seasonality=True,
        weekly_seasonality=True,
        daily_seasonality=False,
        **params
    )
    prophet_data = train_df[['date', 'mean_price']].rename(columns={'date': 'ds', 'mean_price': 'y'})
    model.fit(prophet_data)

    # Валидация на последних 30 днях
    future = model.make_future_dataframe(periods=30)
    forecast = model.predict(future)
    val_dates = train_df['date'].max() - pd.DateOffset(days=30)
    mae = mean_absolute_error(
        train_df[train_df['date'] > val_dates]['mean_price'],
        forecast.tail(30)['yhat']
    )
    return mae

train_cols_best.append('new_target')

data['new_target'] = data['new_target'].fillna(value=1.000000)

train_d, test_d = train_test_split(data[train_cols_best], test_size=0.2, random_state = 42)

features = [col for col in train_d.columns if col not in ['new_target']]

sub['date'] = pd.to_datetime(sub['date'])
new_data['date'] = pd.to_datetime(new_data['date'])

# 2. Создание множества уникальных комбинаций (date, id_house) из sub
sub_keys = set(sub[['date', 'id_house']].apply(tuple, axis=1))

# 3. Поиск совпадающих строк в new_data
mask = new_data[['date', 'id_house']].apply(tuple, axis=1).isin(sub_keys)

# 4. Получение индексов
test_index = new_data[mask].index.tolist()
print(f"Найдено {len(test_index)} совпадающих индексов")

def split_indices(new_data, test_index, time_series=False, val_size=0.2, random_seed=42):
    # Получаем все не-тестовые индексы
    non_test = new_data.index.difference(test_index)

    if time_series:
        # Сортировка по дате для временных рядов
        sorted_dates = new_data.loc[non_test].sort_values('date').index
        split_point = int(len(sorted_dates) * (1 - val_size))
        return sorted_dates[:split_point], sorted_dates[split_point:]
    else:
        # Случайное разделение
        return train_test_split(
            non_test,
            test_size=val_size,
            random_state=random_seed,
            shuffle=True
        )

# Использование
train_index, val_index = split_indices(
    new_data=new_data,
    test_index=test_index,
    time_series=True,  # Для временных рядов
    val_size=0.2
)

print(f"Train size: {len(train_index)}")
print(f"Val size: {len(val_index)}")
print(f"Test size: {len(test_index)}")

target_col = 'new_target'  # Или 'new_target' если используете преобразованную целевую
exclude_cols = [target_col]

features = [col for col in train_d.columns if col not in exclude_cols]

# 2. Создание выборок
X_train = train_d.loc[train_index, features]
y_train = train_d.loc[train_index, target_col]

X_val = test_d.loc[val_index, features]
y_val = test_d.loc[val_index, target_col]

#{'iterations': 1381, 'depth': 10, 'learning_rate': 0.03374541958964185, 'l2_leaf_reg': 9.110469859305407, 'random_strength': 0.1481241935364265, 'bagging_temperature': 9.216998599698078}

# Оптимизация CatBoost
study_cat = optuna.create_study(direction='minimize', sampler=TPESampler())
study_cat.optimize(lambda trial: optimize_catboost(trial, X_train, y_train, X_test, y_test), n_trials=30)

best_cat = CatBoostRegressor(**study_cat.best_params).fit(X_train, y_train)

best_cat.save_model('catboost_model.bin')

preds = best_cat.predict(new_data[selected_features].values[test_index]) * data[f'mean_price_shift_1'][test_index]

data.loc[test_index, 'preds'] = preds

data

sub = sub.merge(data[['date', 'id_house', 'preds']], on = ['date', 'id_house'], how = 'left')
sub

sub['target'] = sub['preds']
# delete temp files of catboost from working folder
sub[['id', 'target']].to_csv('solution.csv', index = None)
sub['target'].min(), sub['target'].isnull().sum()

sub

"""#Pycaret"""

! pip install pycaret
from pycaret.regression import *

import pandas as pd
from sklearn.model_selection import TimeSeriesSplit

# 1. Подготовка данных
def prepare_data(data, m_predict=1):
    # Создаем временные признаки
    data['year'] = data['date'].dt.year
    data['month'] = data['date'].dt.month

    # Генерация лагов и скользящих средних
    for col in ['build_year_median', 'mean_price', 'num_builds_live']:
        data[f'{col}_shift_{m_predict}'] = data.groupby('id_house')[col].shift(m_predict)
        data[f'{col}_rolling_3'] = data.groupby('id_house')[col].transform(lambda x: x.rolling(3).mean())

    data['target'] = data['mean_price'] / data[f'mean_price_shift_{m_predict}']
    return data.dropna()

setup(new_data,
      target='new_target',
      session_id=42,
      use_gpu=True)

best_model = compare_models()

# Список моделей: CatBoost, LightGBM и Prophet для временных рядов
models = ['catboost', 'lightgbm', 'et']

# Сравнение моделей
best = compare_models(include=models,
                      sort='MAE',
                      n_select=3)

# Тюнинг лучшей модели
tuned_models = [tune_model(m, optimize='MAE') for m in best]

blended = blend_models(models, optimize='MAE')
final_model = finalize_model(blended)

# 6. Прогнозирование
predictions = predict_model(final_model, data=test_data)

new_data.to_csv('data.csv', index=False)

from google.colab import files
files.download('data.csv')

catboost = create_model('gbc')

tuned_catboost = tune_model(catboost, optimize='F1', n_iter=20)

test_data_transformed = predict_model(tuned_catboost, data=Test_data.drop(columns='index'))
print(test_data_transformed)