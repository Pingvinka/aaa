{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Rules\n",
        "\n",
        "1. All mathematical expressions should be written in **LaTeX** for better clarity and formatting.\n",
        "2. Ensure that the entire notebook can execute seamlessly from start to finish without encountering errors.\n",
        "3. Focus on optimizing the runtime of the code wherever possible to enhance performance.\n",
        "\n",
        "## Notation\n",
        "\n",
        "- $c$: The optimal constant model.  \n",
        "- $y_i$: The target values in the dataset.  \n",
        "- $w_i$: The weights associated with the loss function.\n",
        "- $q$: Quantile value in range $[0, 1]$.\n",
        "\n",
        "# Важно! О формате сдачи\n",
        "\n",
        "* **При решении ноутбука используйте данный шаблон. Не нужно удалять текстовые ячейки c разметкой частей ноутбука и формулировками заданий. Добавлять свои ячейки, при необходимости, конечно можно**\n",
        "* **Везде, где в формулровке задания есть какой-либо вопрос (или просьба вывода), необходимо прописать ответ в ячейку (код или markdown).**\n",
        "* **Наличие кода решения (или аналитического решения - в зависимости от задачи) обязательно. Письменные ответы на вопросы без сопутствующего кода/аналитического решения оцениваются в 0 баллов.**"
      ],
      "metadata": {
        "id": "GOb1Kh1lj609"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1"
      ],
      "metadata": {
        "id": "Fr4xVEQsjeF9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Description"
      ],
      "metadata": {
        "id": "jfx6HJOj5FLb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the loss function:\n",
        "\n",
        "\\begin{align}\n",
        "L = \\sum w_i \\cdot \\left( \\log(y_i) - \\log(c) \\right)^2\n",
        "\\end{align}\n",
        "\n",
        "where:\n",
        "\n",
        "- $\\sum w_i = 1$\n",
        "\n",
        "#### Tasks\n",
        "\n",
        "1. **Analytically find the best constant $c$** for the given loss function.\n",
        "2. **Determine the name of the aggregation of $y_i$'s** at the end if $w_1 = w_2 = \\dots = w_n$."
      ],
      "metadata": {
        "id": "QQFbyGOo7Y9p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "QujLaGbD5A8E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$L = \\sum_{i=1}^{n} w_i \\cdot (\\log(y_i) - \\log(c))^2$\n",
        "where $\\sum_{i=1}^{n} w_i = 1$.\n",
        "\n",
        "Найдем производную L от c.\n",
        "\n",
        "$\\frac{dL}{dz} = \\sum_{i=1}^{n} w_i \\cdot 2 \\cdot (\\log(y_i) - \\log(c)) \\cdot (-1)$\n",
        "\n",
        "$\\frac{dL}{dz} = -2 \\sum_{i=1}^{n} w_i \\cdot (\\log(y_i) - \\log(c))$\n",
        "\n",
        "Приравниваем к нулю.\n",
        "\n",
        "$-2 \\sum_{i=1}^{n} w_i \\cdot (\\log(y_i) - \\log(c)) = 0$\n",
        "\n",
        "Делим обе части уравнения на -2.\n",
        "\n",
        "$\\sum_{i=1}^{n} w_i \\cdot (\\log(y_i) - \\log(c)) = 0$\n",
        "\n",
        "Раскрываем скобки.\n",
        "\n",
        "$\\sum_{i=1}^{n} w_i \\cdot \\log(y_i) - \\sum_{i=1}^{n} w_i \\cdot \\log(c) = 0$\n",
        "\n",
        "\n",
        "Так как $\\sum_{i=1}^{n} w_i = 1$:\n",
        "\n",
        "$\\sum_{i=1}^{n} w_i \\cdot \\log(y_i) - \\log(c) = 0$\n",
        "\n",
        "$\\log(c) = \\sum_{i=1}^{n} w_i \\cdot \\log(y_i)$\n",
        "\n",
        "Итак,\n",
        "\n",
        "$c = \\exp\\left(\\sum_{i=1}^{n} w_i \\cdot \\log(y_i)\\right)$\n",
        "\n",
        "$\\sum_{i=1}^{n} w_i \\cdot \\log(y_i) = \\log\\left(\\prod_{i=1}^{n} y_i^{w_i}\\right)$\n",
        "\n",
        "$c = \\prod_{i=1}^{n} y_i^{w_i}$\n",
        "\n",
        "Оптимальной константой $c$ является взвешенное среднее геометрическое значений $y_i$.\n",
        "\n",
        "Если $w_i = \\frac{1}{n}$, то:\n",
        "\n",
        "$c = \\prod_{i=1}^{n} y_i^{\\frac{1}{n}} = \\left(\\prod_{i=1}^{n} y_i\\right)^{\\frac{1}{n}}$\n",
        "\n",
        "Это среднее геометрическое значений $y_i$.\n",
        "---\n",
        "\n",
        "### Final Answer\n",
        "1) $c = \\prod_{i=1}^{n} y_i^{w_i}$\n",
        "\n",
        "2) Среднее геометрическое значений $y_i$"
      ],
      "metadata": {
        "id": "1hVENP-85OaX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 2"
      ],
      "metadata": {
        "id": "iXSZRuzZ7Mnt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Description"
      ],
      "metadata": {
        "id": "A14Tq1s470G6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the **quantile loss function** $L$, and prove that the optimal constant $c$ corresponds to the quantile $q$ of the data $y_1, \\dots, y_n$.\n",
        "\n",
        "The quantile loss function is defined as:\n",
        "\n",
        "\\begin{align}\n",
        "L =\n",
        "\\begin{cases}\n",
        "q \\cdot (y_i - c), & \\text{if } y_i \\geq c \\\\\n",
        "(1-q) \\cdot (c - y_i), & \\text{if } y_i < c\n",
        "\\end{cases}\n",
        "\\end{align}\n",
        "\n",
        "**Hint**:\n",
        "\n",
        "Proof for optimal MAE constant on [this page](https://ds100.org/course-notes/constant_model_loss_transformations/loss_transformations.html).\n",
        "\n"
      ],
      "metadata": {
        "id": "vTM5F2rZ78Zm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "HohX1KQk70Jd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "L(c) = \\sum_{i=1}^n \\begin{cases} q \\cdot (y_i - c), & \\text{if } y_i \\ge c \\\\ (1 - q) \\cdot (c - y_i), & \\text{if } y_i < c \\end{cases}\n",
        "$$\n",
        "\n",
        "Нужно минимизировать $L(c)$, возьмем производную.\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial c} = \\sum_{i=1}^n \\begin{cases} -q, & \\text{if } y_i \\ge c \\\\ (1 - q), & \\text{if } y_i < c \\end{cases}\n",
        "$$\n",
        "\n",
        "Приравняем к нулю.\n",
        "\n",
        "$$\n",
        "\\sum_{i: y_i \\ge c} (-q) + \\sum_{i: y_i < c} (1 - q) = 0.\n",
        "$$\n",
        "Так как $n_{\\ge c} + n_{<c} = n$, подставим $n_{<c} = n - n_{\\ge c}$:\n",
        "\n",
        "$$\n",
        "-q \\cdot n_{\\ge c} + (1 - q) \\cdot (n - n_{\\ge c}) = 0.\n",
        "$$\n",
        "\n",
        "Упростим:\n",
        "\n",
        "$$\n",
        "-n_{\\ge c}q + (1 - q)n - (1 - q)n_{\\ge c} = 0.\n",
        "$$\n",
        "\n",
        "Вынесем $n_{\\ge c}$:\n",
        "\n",
        "$$\n",
        "n_{\\ge c}\\big(-q - (1 - q)\\big) + (1 - q)n = 0.\n",
        "$$\n",
        "\n",
        "Упростим коэффициенты:\n",
        "\n",
        "$$\n",
        "-n_{\\ge c} + (1 - q)n = 0 \\quad \\implies \\quad n_{\\ge c} = (1 - q)n.\n",
        "$$\n",
        "\n",
        "Следовательно:\n",
        "$$\n",
        "\\frac{n_{<\\,c}}{n} = q\n",
        "$$\n",
        "\n",
        "\n",
        "Следовательно, c — это значение, ниже которого лежит доля q данных. Это в точности соответствует определению q-квантили.\n",
        "\n",
        "---\n",
        "\n",
        "### Final Answer\n",
        "\n",
        "Оптимальная константа c, минимизирующая квантильную функцию потерь, совпадает с q-квантилью данных y1, …, yn."
      ],
      "metadata": {
        "id": "hoMeumwu7RGc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Problem 3"
      ],
      "metadata": {
        "id": "VKg7Sh-H_XlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def bruteforce_constant(y: np.ndarray, loss_func: callable, tol: float) -> float:\n",
        "    \"\"\"\n",
        "    Finds the optimal constant c by brute force using a specified loss function.\n",
        "\n",
        "    Parameters:\n",
        "    y (np.ndarray): Array of target values.\n",
        "    loss_func (callable): A function that computes the loss given y and c.\n",
        "    tol (float): The step size for generating potential c values between min(y) and max(y).\n",
        "\n",
        "    Returns:\n",
        "    float: The optimal constant c that minimizes the loss.\n",
        "    \"\"\"\n",
        "\n",
        "    c_values = np.arange(np.min(y), np.max(y), tol)\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    best_c = None\n",
        "    for c in c_values:\n",
        "        current_loss = loss(y, c)\n",
        "        if current_loss < best_loss:\n",
        "            best_loss = current_loss\n",
        "            best_c = c\n",
        "\n",
        "    return best_c\n",
        "\n",
        "def loss(y, c):\n",
        "    return np.sum(np.log(np.cosh(y - c)))\n",
        "\n",
        "y = np.array([1, 2, 3, 4, 55, 99, 100])\n",
        "optimal_c = bruteforce_constant(y, loss, tol=0.001)\n",
        "min_loss = loss(y, optimal_c)\n",
        "print(f\"The optimal constant c is: {optimal_c}\")\n",
        "print(f\"The minimum loss is: {min_loss}\")"
      ],
      "metadata": {
        "id": "gsCBQe8C-U-i",
        "outputId": "b11edfd6-a045-45af-cf89-790727345f90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The optimal constant c is: 4.1969999999996475\n",
            "The minimum loss is: 243.96167948452023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 4"
      ],
      "metadata": {
        "id": "H1UPKOpBLb5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.optimize import minimize\n",
        "import numpy as np\n",
        "\n",
        "def minimize_constant(y: np.ndarray, loss_func: callable, tol: float) -> float:\n",
        "    \"\"\"\n",
        "    Finds the optimal constant c using scipy's minimize function with a specified loss function.\n",
        "\n",
        "    Parameters:\n",
        "    y (np.ndarray): Array of target values.\n",
        "    loss_func (callable): A function that computes the loss given y and c.\n",
        "    tol (float): The tolerance for the optimization process.\n",
        "\n",
        "    Returns:\n",
        "    float: The optimal constant c that minimizes the loss.\n",
        "    \"\"\"\n",
        "    initial_guess = np.mean(y)\n",
        "    def loss_wrapper(c):\n",
        "        return loss(y, c)\n",
        "\n",
        "    result = minimize(loss_wrapper, initial_guess, tol=tol)\n",
        "\n",
        "    return result.x[0]\n",
        "\n",
        "def loss(y, c):\n",
        "    return np.sum(np.log(np.cosh(y - c)))\n",
        "\n",
        "y = np.array([1, 2, 3, 4, 55, 99, 100])\n",
        "optimal_c = minimize_constant(y, loss, tol=0.001)\n",
        "min_loss = loss(y, optimal_c)\n",
        "print(f\"The optimal constant c is: {optimal_c}\")\n",
        "print(f\"The minimum loss is: {min_loss}\")"
      ],
      "metadata": {
        "id": "NI-R1YHaIRx_",
        "outputId": "89ab8375-088b-4711-8738-92c5aae01290",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The optimal constant c is: 4.197181683003827\n",
            "The minimum loss is: 243.96167941355066\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 5\n",
        "\n",
        "## Description\n",
        "In a multiclass classification problem, we often compare a model’s performance with a simple baseline. Two baseline approaches are:\n",
        "\n",
        "1. **Always predict the most frequent class** (also known as the “constant baseline”).  \n",
        "2. **Randomly sample a class** for every object with probabilities proportional to the class frequencies in the training set.\n",
        "\n",
        "The goal of this task is to determine which of these two approaches yields a higher **Accuracy** on a given dataset.\n",
        "\n",
        "You are provided with:  \n",
        "1. A set of objects \\\\( X \\\\) and their true class labels \\\\( y \\\\), where \\\\( y \\in \\{C_1, C_2, \\dots, C_k\\} \\\\).  \n",
        "2. The frequencies (or proportions) of each class in the training data.\n",
        "\n",
        "You need to:  \n",
        "1. **Explain** how to implement these two baseline approaches (always predicting the most frequent class vs. sampling a class according to its frequency).  \n",
        "2. **Calculate** the expected Accuracy for each approach.  \n",
        "3. **Compare** the results to determine which method is more likely to produce a higher Accuracy."
      ],
      "metadata": {
        "id": "xBJbvAj27Bld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Solution\n",
        "import numpy as np\n",
        "def constant_base(class_freq):\n",
        "  return np.max(class_freq)\n",
        "def random_by_prop(class_freq):\n",
        "  return np.sum(class_freq ** 2)\n",
        "def sravnenie_bases(class_frequencies):\n",
        "  constant_baseline_accuracy = constant_base(class_frequencies)\n",
        "  random_sampling_accuracy = random_by_prop(class_frequencies)\n",
        "  print(f\"Ожидаемая точность (с константой) пример 1: {constant_baseline_accuracy:.4f}\")\n",
        "  print(f\"Ожидаемая точность (случайный выбор) пример 2: {random_sampling_accuracy:.4f}\")\n",
        "\n",
        "  if constant_baseline_accuracy > random_sampling_accuracy:\n",
        "      result = \"Всегда предсказывать самый частый класс лучше.\"\n",
        "  elif constant_baseline_accuracy == random_sampling_accuracy:\n",
        "      result = \"Методы равны по точности.\"\n",
        "  else:\n",
        "      result = \"Случайный выбор лучше.\"  # Спойлер: такого не будет\n",
        "\n",
        "  return result\n",
        "class_frequencies = np.array([0.5, 0.3, 0.2])\n",
        "result = sravnenie_bases(class_frequencies)\n",
        "print(result)\n",
        "print('\\n')\n",
        "class_frequencies = np.array([0.92, 0.08])\n",
        "result = sravnenie_bases(class_frequencies)\n",
        "print(result)\n",
        "print('\\n')\n",
        "class_frequencies = np.array([0.5, 0.5])\n",
        "result = sravnenie_bases(class_frequencies)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkFGSAG2cpOZ",
        "outputId": "bd0f4113-cc54-48cd-b31d-21c80f2f538f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ожидаемая точность (с константой) пример 1: 0.5000\n",
            "Ожидаемая точность (случайный выбор) пример 2: 0.3800\n",
            "Всегда предсказывать самый частый класс лучше.\n",
            "\n",
            "\n",
            "Ожидаемая точность (с константой) пример 1: 0.9200\n",
            "Ожидаемая точность (случайный выбор) пример 2: 0.8528\n",
            "Всегда предсказывать самый частый класс лучше.\n",
            "\n",
            "\n",
            "Ожидаемая точность (с константой) пример 1: 0.5000\n",
            "Ожидаемая точность (случайный выбор) пример 2: 0.5000\n",
            "Методы равны по точности.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Почему же это так работает?\n",
        "\n",
        "В первом решении всё понятно — просто предсказываем самый частый класс. Соответственно, Accuracy (ну так не со всеми метриками будет хорошо работать :)) будет равен как раз частоте появления этого класса в выборке (но не факт, что в тесте такое же распределение по классам). Соответственно,\n",
        "\n",
        "$$\n",
        "\\text{Accuracy} = p_{max}\n",
        "$$\n",
        "\n",
        "Во втором подходе для каждого объекта мы случайным образом выбираем класс в зависимости от его частоты. Ожидаемая точность этого метода рассчитывается так:\n",
        "\n",
        "$$\n",
        "\\text{Accuracy} = \\sum_{i=1}^k p_i^2,\n",
        "$$\n",
        "\n",
        "где $p_i$ — это доля объектов класса $C_i$.\n",
        "\n",
        "Поскольку $p_{max}$ — это максимальная из всех $p_i$, то для каждого $i$, $p_i \\le p_{max}$. Тогда $p_i^2 \\le p_{max} \\cdot p_i$, потому что если $p_i \\le p_{max}$, умножение обеих сторон на $p_i$ (неотрицательное число) сохранит неравенство.\n",
        "\n",
        "Теперь суммируем неравенства $p_i^2 \\le p_{max} \\cdot p_i$ для всех $i$ от 1 до $k$ (количества классов):\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^k p_i^2 \\le \\sum_{i=1}^k (p_{max} \\cdot p_i)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^k p_i^2 \\le p_{max} \\sum_{i=1}^k p_i\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^k p_i^2 \\le p_{max} \\cdot 1\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^k p_i^2 \\le p_{max}\n",
        "$$\n",
        "\n",
        "Ч. Т. Д. :>"
      ],
      "metadata": {
        "id": "3jKBwO0CgwUP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final answer\n",
        "\n",
        "Always predict the most frequent class is better than Randomly sample a class for every object with probabilities proportional to the class frequencies in the training set\n",
        "(Всегда предсказывать наиболее частый класс лучше, чем рандомно выбирать класс для каждого объекта с вероятностями, пропорциональными частотам классов в обучающем наборе)."
      ],
      "metadata": {
        "id": "Cabuv0Ti79qz"
      }
    }
  ]
}